{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec242b2d-6b1a-4fdf-ac3d-40fc1946165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U -q git+https://github.com/huggingface/diffusers\n",
    "# !pip install -q -U diffusers==0.30.3\n",
    "# !pip install transformers -U -q\n",
    "# !pip install -U -q tokenizers==0.20\n",
    "# !pip install -q- U controlnet_aux\n",
    "# !pip install accelerate==0.32.1 -U\n",
    "# !pip install mediapipe\n",
    "# ! pip install protobuf==3.20\n",
    "# !pip install sentencepiece -U\n",
    "# !pip install torch -U -q\n",
    "# !pip install -q -U quanto\n",
    "# !pip install optimum\n",
    "# !git clone https://github.com/Modegen/flux-controlnet-inpaint.git external_deps/\n",
    "# !git clone https://github.com/Modegen/ControlNetPlus.git external_deps/\n",
    "# !export GH_PAT=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\n",
    "# !git clone https://${GH_PAT}@github.com/Modegen/LookBuilderPipeline.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229f6b77-49ae-4b2c-8fb3-09109b9c9f84",
   "metadata": {},
   "source": [
    "**diffusers must be version 0.31.0dev0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bac5ce0-1bb2-436f-866a-4896cf0e040e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.8/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mdiffusers                 0.31.0.dev0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.8/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list|grep diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62650602-1bc5-48b6-aed0-f3b00e1b1794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.8/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "/usr/local/lib/python3.8/dist-packages/controlnet_aux/open_pose/body.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_dict = util.transfer(self.model, torch.load(model_path))\n",
      "/usr/local/lib/python3.8/dist-packages/controlnet_aux/open_pose/hand.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_dict = util.transfer(self.model, torch.load(model_path))\n",
      "/usr/local/lib/python3.8/dist-packages/controlnet_aux/open_pose/face.py:325: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(face_model_path))\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.insert(2,os.path.abspath('LookBuilderPipeline/LookBuilderPipeline/'))\n",
    "\n",
    "import torch\n",
    "# import torch.nn as nn\n",
    "from diffusers.utils import load_image\n",
    "from diffusers.pipelines.flux import FluxControlNetInpaintPipeline\n",
    "from diffusers.models import FluxControlNetModel\n",
    "from diffusers.utils import load_image, check_min_version\n",
    "from transformers import pipeline ,SegformerImageProcessor, AutoModelForSemanticSegmentation\n",
    "\n",
    "import glob\n",
    "from time import time\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from controlnet_aux import OpenposeDetector\n",
    "from pathlib import Path\n",
    "\n",
    "from LookBuilderPipeline.LookBuilderPipeline.resize import resize_images\n",
    "from LookBuilderPipeline.LookBuilderPipeline.segment import segment_image\n",
    "from LookBuilderPipeline.LookBuilderPipeline.pose import detect_pose\n",
    "from LookBuilderPipeline.plot_images import showImagesHorizontally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54da4627-bf89-4351-ba09-240ace115cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionXLInpaintPipeline,ControlNetModel,StableDiffusionXLControlNetInpaintPipeline\n",
    "from transformers import pipeline \n",
    "segmenter = pipeline(model=\"mattmdjaga/segformer_b2_clothes\")#,device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e875833-839a-4c94-a46a-6398689d513d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33650c1d2d7246dd94b87816cc134075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "from diffusers import FlowMatchEulerDiscreteScheduler, AutoencoderKL\n",
    "from diffusers.models.transformers.transformer_flux import FluxTransformer2DModel\n",
    "from diffusers.pipelines.flux.pipeline_flux import FluxPipeline\n",
    "from transformers import CLIPTextModel, CLIPTokenizer,T5EncoderModel, T5TokenizerFast\n",
    "dtype = torch.bfloat16\n",
    "scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained('flux-fp8', subfolder=\"scheduler\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=dtype)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=dtype)\n",
    "text_encoder_2 = T5EncoderModel.from_pretrained('flux-fp8', subfolder=\"text_encoder_2\", torch_dtype=dtype)\n",
    "tokenizer_2 = T5TokenizerFast.from_pretrained('flux-fp8', subfolder=\"tokenizer_2\", torch_dtype=dtype)\n",
    "vae = AutoencoderKL.from_pretrained('flux-fp8', subfolder=\"vae\", torch_dtype=dtype)\n",
    "transformer = FluxTransformer2DModel.from_pretrained('flux-fp8', subfolder=\"transformer\", torch_dtype=dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7c1d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from quanto import quantize, freeze, qint4, qint8, qfloat8\n",
    "\n",
    "quantize(transformer, weights=qfloat8)\n",
    "freeze(transformer)\n",
    "\n",
    "quantize(text_encoder_2, weights=qfloat8)\n",
    "freeze(text_encoder_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0aecdf-d312-43c8-ba8b-11f6029aa598",
   "metadata": {},
   "source": [
    "**load initial flux pipe for talent generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d59ed4c-d79f-4ec1-9e78-f47c2fef79ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint were not used when initializing FluxControlNetModel: \n",
      " ['time_text_embed.guidance_embedder.linear_1.bias, time_text_embed.guidance_embedder.linear_1.weight, time_text_embed.guidance_embedder.linear_2.bias, time_text_embed.guidance_embedder.linear_2.weight']\n"
     ]
    }
   ],
   "source": [
    "controlnet_model = 'InstantX/FLUX.1-dev-Controlnet-Union'\n",
    "controlnet = FluxControlNetModel.from_pretrained(controlnet_model,use_safetensors=True, torch_dtype=torch.bfloat16, add_prefix_space=True,local_files_only=True,guidance_embeds=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d23ddae-17e3-46cd-8163-40f8aac055a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783bcf0ed23040aa935344851d3d3d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = FluxControlNetInpaintPipeline.from_pretrained(\"ostris/OpenFLUX.1\",\n",
    "            controlnet=controlnet,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            scheduler=scheduler,\n",
    "            text_encoder=text_encoder,\n",
    "            tokenizer=tokenizer,\n",
    "            text_encoder_2=None,\n",
    "            tokenizer_2=tokenizer_2,\n",
    "            vae=vae,\n",
    "            transformer=None,)\n",
    "\n",
    "pipe.text_encoder_2 = text_encoder_2\n",
    "pipe.transformer = transformer\n",
    "# pipe.enable_model_cpu_offload()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dda1275-a252-4a06-8443-f36f6544dfdc",
   "metadata": {},
   "source": [
    "**load secondary sdxl pipe for background generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c9729ee-639e-40fa-b8e0-6953ce4adcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# controlnet_model = ControlNetModel.from_pretrained(\"controlnet-union-sdxl-1.0-promax\", torch_dtype=torch.float16, use_safetensors=True,)\n",
    "# pipe2 = StableDiffusionXLControlNetInpaintPipeline.from_pretrained(\n",
    "#     \"RunDiffusion/Juggernaut-XL-v8\", controlnet=controlnet_model,\n",
    "#     torch_dtype=torch.float16,\n",
    "# )\n",
    "\n",
    "# pipe2.text_encoder.to(torch.float16)\n",
    "# pipe2.controlnet.to(torch.float16)\n",
    "# # pipe.to(\"cuda\")\n",
    "# pipe2.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17609656-3ead-4570-8dea-4728c5f9f98e",
   "metadata": {},
   "source": [
    "**few ugly helper functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04e7705a-d821-44ea-b38f-e445b37a4c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from LookBuilderPipeline.LookBuilderPipeline.segment import full_mask\n",
    "# from LookBuilderPipeline.plot_images import showImagesHorizontally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae5e12e-a346-4e6e-8f92-5bf2ea076cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from time import time\n",
    "from random import shuffle\n",
    "dir=glob.glob('LookBuilderPipeline/LookBuilderPipeline/img/paulo_orig/*')\n",
    "dir+=glob.glob('LookBuilderPipeline/LookBuilderPipeline/img/orig/*')\n",
    "\n",
    "shuffle(dir)\n",
    "\n",
    "\n",
    "for input_image in dir:\n",
    "    for j in [100,101,102]:\n",
    "        image = load_image(input_image)\n",
    "        pose_image = detect_pose(input_image)\n",
    "        mask,mask_image,mask_array = segment_image(input_image,inverse=True,additional_option='shoe')\n",
    "        mask2,mask_image2,mask_array2 = segment_image(input_image,inverse=False,additional_option='shoe')\n",
    "        \n",
    "        if image.size[0] < image.size[0]:\n",
    "            sm_image=resize_images(image,sm_pose_image.size,aspect_ratio=sm_pose_image.size[0]/sm_pose_image.size[1])\n",
    "            sm_pose_image=resize_images(pose_image,sm_pose_image.size,aspect_ratio=None)\n",
    "            sm_mask=resize_images(mask_image,sm_pose_image.size,aspect_ratio=sm_pose_image.size[0]/sm_pose_image.size[1])\n",
    "    \n",
    "        else:\n",
    "            sm_image=resize_images(image,target_size=image.size,aspect_ratio=None)\n",
    "            sm_pose_image=resize_images(pose_image,image.size,aspect_ratio=image.size[0]/image.size[1])\n",
    "            sm_mask=resize_images(mask_image,image.size,aspect_ratio=image.size[0]/image.size[1])\n",
    "    \n",
    "        width,height=sm_image.size\n",
    "        prompt=\"photo realistic african fashion model with black skin under eiffel tower\"\n",
    "        negative_prompt=\"ugly, bad quality, bad anatomy, deformed body, deformed hands, deformed feet, deformed face, deformed clothing, deformed skin, bad skin, leggings, tights, stockings, pants, sleeves\",\n",
    "        t=time()\n",
    "        num_inference_steps=30\n",
    "        guidance_scale=3.5\n",
    "        controlnet_conditioning_scale=0.7\n",
    "        # seed=np.random.randint(0,100000000)\n",
    "        seed=j#42\n",
    "        generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
    "        image_res = pipe(\n",
    "                prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                image=sm_image,\n",
    "                control_image=sm_pose_image,\n",
    "                control_mode=4,\n",
    "                padding_mask_crop=32,\n",
    "                controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "                mask_image=sm_mask,\n",
    "                height=height,\n",
    "                width=width,\n",
    "                strength=0.7,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                guidance_scale=guidance_scale,\n",
    "                generator=generator,\n",
    "                # prompt_embeds=embeds,\n",
    "                # pooled_prompt_embeds=embeds,\n",
    "            ).images[0]\n",
    "        tt=time()-t\n",
    "        \n",
    "        # filename='openflux_test_output/flux_'+str(seed)+os.path.basename(input_image)+'.png'\n",
    "        # filename2='openflux_raw_out/flux_'+str(seed)+os.path.basename(input_image)+'.png'\n",
    "        # image_res.save(filename2)\n",
    "        # mask2,mask_image2,mask_array2 = segment_image('test.png',inverse=True,additional_option='shoe')\n",
    "        # sm_mask3=resize_images(mask_image,mask_image2.size,aspect_ratio=mask_image2.size[0]/mask_image2.size[1])\n",
    "    \n",
    "        # print('mask shape is same:',np.array(sm_mask3).shape==mask_array2.shape)\n",
    "        # cov=(np.sum(np.array(sm_mask3)==mask_array2))/np.array(sm_mask).size\n",
    "        # valid_pixels = (np.array(sm_mask3) < 10)  # Create a mask for pixels with values < 10\n",
    "        # fcov = np.sum((np.array(sm_mask3) == mask_array2) & valid_pixels) / np.sum(valid_pixels)  # Calculate weighted coverage\n",
    "    \n",
    "        # showImagesHorizontally([sm_image,sm_mask,sm_pose_image,image_res],prompt,np.round(tt,2),np.round(cov,2),np.round(fcov,2),'flux',height, width,controlnet_conditioning_scale,num_inference_steps,guidance_scale,seed,filename)\n",
    "    \n",
    "        # final_mask=full_mask(sm_image,image_res)\n",
    "\n",
    "        # mask = Image.new(\"L\", final_mask.size)\n",
    "        # mask.paste(final_mask.split()[3], (x, y))\n",
    "        # mask = ImageOps.invert(mask)\n",
    "        # final_mask = mask.point(lambda p: p > 128 and 255)\n",
    "        # mask_blurred = pipeline.mask_processor.blur(final_mask, blur_factor=20)\n",
    "        \n",
    "        # mask_blurred.save('mask.png')\n",
    "        # final_mask=load_image('mask.png').convert(\"RGB\")\n",
    "        # image_res=image_res.resize(sm_pose_image.size)\n",
    "        # final_mask=final_mask.resize(sm_pose_image.size) ## just forcing it for now -- will fix this inline with our codebase in future\n",
    "        \n",
    "        # for i,prompt2 in enumerate([\"photo realistic small naked bald person on paris street corner\",\n",
    "        #                           \"photo realistic small naked bald person on a wooden sailing vessel at sunset\",\n",
    "        #                           \"photo realistic small naked bald person on a a beach at sunset\",\n",
    "        #                           \"photo realistic small naked bald person in a candle-lit thai temple\"]):\n",
    "        #     image = pipe2(prompt=prompt2,\n",
    "        #                  image=(image_res),\n",
    "        #                  mask_image=(final_mask),\n",
    "        #                  # padding_mask_crop=32,\n",
    "        #                  control_image=(sm_pose_image),\n",
    "        #                  num_inference_steps=50,\n",
    "        #                  strength=0.80\n",
    "        #         ).images[0]\n",
    "        #     image.save('openflux_raw_out/flux_xl_p1='+str(j)+'_p2='+str(i)+os.path.basename(input_image))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1967c7e0-459e-49d2-add7-6b122fb057e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_res#.save('OF_fp8_test2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be954c51-b5d5-457a-abf8-66c20f569068",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e9fc20-793a-4090-b254-3c5c66a2b63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.save('image_in.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec242b2d-6b1a-4fdf-ac3d-40fc1946165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install diffusers\n",
    "# !pip install -U -q git+https://github.com/huggingface/diffusers\n",
    "# !pip install transformers -U -q\n",
    "# !pip install -U tokenizers==0.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62650602-1bc5-48b6-aed0-f3b00e1b1794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('/flux-controlnet-inpaint/src'))\n",
    "import torch\n",
    "from diffusers.utils import load_image\n",
    "from diffusers.pipelines.flux.pipeline_flux_controlnet_inpaint import FluxControlNetInpaintPipeline\n",
    "from diffusers.models.controlnet_flux import FluxControlNetModel\n",
    "from diffusers import FluxMultiControlNetModel\n",
    "from diffusers.utils import load_image, check_min_version\n",
    "from PIL import Image\n",
    "# import cv2\n",
    "import numpy as np\n",
    "from transformers import SegformerImageProcessor, AutoModelForSemanticSegmentation\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from controlnet_aux import HEDdetector, MidasDetector, MLSDdetector, OpenposeDetector, PidiNetDetector, NormalBaeDetector, LineartDetector, LineartAnimeDetector, CannyDetector, ContentShuffleDetector, ZoeDetector, MediapipeFaceDetector, SamDetector, LeresDetector, DWposeDetector\n",
    "import os\n",
    "from pathlib import Path\n",
    "# from inpaint import get_mask\n",
    "\n",
    "import torch\n",
    "from diffusers.utils import load_image\n",
    "from diffusers.pipelines.flux.pipeline_flux_controlnet_inpaint import FluxControlNetInpaintPipeline\n",
    "from diffusers.models.controlnet_flux import FluxControlNetModel\n",
    "from controlnet_aux import CannyDetector\n",
    "from transformers import pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d1346d1-71df-444a-87f0-ad54ebbef3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure, imshow, axis\n",
    "from matplotlib.image import imread\n",
    "\n",
    "\n",
    "def showImagesHorizontally(list_of_files, output_path='output.png'):\n",
    "    fig = figure()\n",
    "    number_of_files = len(list_of_files)\n",
    "    for i in range(number_of_files):\n",
    "        a=fig.add_subplot(1,number_of_files,i+1)\n",
    "        image = (list_of_files[i])\n",
    "        imshow(image,cmap='Greys_r')\n",
    "        axis('off')\n",
    "    plt.tight_layout()  # Adjust the layout to prevent overlapping\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')  # Save the figure\n",
    "    plt.close(fig)  # Close the figure to free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "282b3ff7-415c-4722-81ea-7c5aeef731b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487eb41a46d44ceab05aa975859355f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fea4054ad11413f88cb05e32f5c2dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "base_model = 'black-forest-labs/FLUX.1-dev'\n",
    "controlnet_model2 = 'InstantX/FLUX.1-dev-Controlnet-Union'\n",
    "\n",
    "controlnet_pose = FluxControlNetModel.from_pretrained(controlnet_model2, torch_dtype=torch.float16)\n",
    "controlnet = FluxMultiControlNetModel([controlnet_pose,controlnet_pose])\n",
    "\n",
    "pipe = FluxControlNetInpaintPipeline.from_pretrained(base_model, controlnet=controlnet, torch_dtype=torch.float16)\n",
    "pipe.enable_model_cpu_offload() #save some VRAM by offloading the model to CPU. Remove this if you have enough GPU power\n",
    "# pipe.to(\"cuda\")\n",
    "\n",
    "pipe.text_encoder.to(torch.float16)\n",
    "pipe.controlnet.to(torch.float16)\n",
    "pipe.enable_sequential_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35c51ad5-3be3-41be-a427-effe277af750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running canny\n"
     ]
    }
   ],
   "source": [
    "image = load_image('/p12.jpg').resize((1024, 1024))\n",
    "openpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\n",
    "pose_image = openpose(image)\n",
    "\n",
    "print(\"running segmentation\")\n",
    "segmenter = pipeline(model=\"mattmdjaga/segformer_b2_clothes\")\n",
    "segments = segmenter(image)\n",
    "segment_include = [\"Upper-clothes\", \"Skirt\", \"Pants\", \"Dress\", \"Belt\", \"Bag\", \"Scarf\", \"Right-shoe\",\"Left-shoe\",\"Bag\"]\n",
    "mask_list = [np.array(s['mask']) for s in segments if s['label'] not in segment_include]\n",
    "final_mask = np.array(mask_list[0])\n",
    "\n",
    "for mask in mask_list:\n",
    "    current_mask = np.array(mask)\n",
    "    final_mask = final_mask + current_mask  # Add the current mask to the final mask\n",
    "    \n",
    "final_array = final_mask.copy() \n",
    "final_mask = Image.fromarray(final_mask)\n",
    "mask=final_mask.resize((1024, 1024))\n",
    "\n",
    "label=str(np.random.randint(100000000))\n",
    "\n",
    "print(\"running canny\")\n",
    "canny = CannyDetector()\n",
    "canny_image = canny(image)\n",
    "\n",
    "image=image.resize((1024, 1024))\n",
    "mask=mask.resize((1024, 1024))\n",
    "\n",
    "\n",
    "showImagesHorizontally([image,mask,canny_image,pose_image],'input'+label+'.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca2413-19c1-4874-953e-290ed2a50693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b371696f8634d8ca0aabac66fb099e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "images = []\n",
    "for seed in range(10, 13):\n",
    "    scale = 0.8\n",
    "    prompt = \"clothing model\"   \n",
    "    negative_prompt='ugly, bad quality, bad anatomy, deformed body, deformed hands, deformed feet, deformed face, deformed clothing, deformed skin, bad skin, leggings, sleeves, tights, stockings'\n",
    "         \n",
    "    prompt = prompt + \", indoor\" if random.uniform(0,1)<0.5 else prompt + \", outdoor\"\n",
    "    generator = torch.Generator(device=\"cpu\").manual_seed(seed)\n",
    "    image_res = pipe(\n",
    "        prompt,\n",
    "        negative_prompt,\n",
    "        image=image,\n",
    "        control_image=[pose_image],\n",
    "        control_mode=[4],\n",
    "        controlnet_conditioning_scale=0.8,\n",
    "        mask_image=mask,\n",
    "        strength=0.95,\n",
    "        num_inference_steps=20,\n",
    "        guidance_scale=5,\n",
    "        generator=generator,\n",
    "        joint_attention_kwargs={\"scale\": scale},    \n",
    "    ).images[0]\n",
    "    image_res.resize((1024, 1024)).save('out'+str(seed)+'.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4f9ade-c737-4a12-a677-84d3c6a39dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from diffusers.utils import load_image\n",
    "# from diffusers import FluxControlNetInpaintPipeline #, FluxImg2ImgPipeline\n",
    "from diffusers.models.controlnet_flux import FluxControlNetModel\n",
    "\n",
    "from diffusers.utils import load_image, check_min_version\n",
    "from transformers import pipeline ,SegformerImageProcessor, AutoModelForSemanticSegmentation\n",
    "import os\n",
    "import time\n",
    "import cProfile\n",
    "from pstats import Stats\n",
    "# from optimum.quanto import qfloat8,qint4,qint8, quantize,freeze\n",
    "from quanto import qfloat8,qint4,qint8, quantize,freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc506f5-893e-4a0d-b112-272d3c7d5a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet_model = 'InstantX/FLUX.1-dev-Controlnet-Union'\n",
    "controlnet = FluxControlNetModel.from_pretrained(controlnet_model, torch_dtype=torch.bfloat16,add_prefix_space=True,guidance_embeds=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c769b7-8bee-4b34-8592-c8c458d62d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# from random import shuffle\n",
    "# # dir=glob.glob('test-ai/orig/*')\n",
    "# # dir+=glob.glob('test-ai/jcrew_orig/*')\n",
    "# # dir=glob.glob('test-ai/paulo_orig/*')\n",
    "# # dir=glob.glob('test-ai/upscaled/*')\n",
    "dir=glob.glob('test-ai/more_paulo/*')\n",
    "\n",
    "# shuffle(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938dbef2-e254-440c-a59d-611ef6151534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12df178-b0fa-4361-8dff-82189711f20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LookBuilderPipeline.resize import resize_images\n",
    "from LookBuilderPipeline.segment import segment_image\n",
    "from LookBuilderPipeline.pose import detect_pose\n",
    "# from LookBuilderPipeline.plot_images import showImagesHorizontally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ac98e0-52bd-44f4-900e-5ec595608b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_setup(input_image):\n",
    "    image = load_image(input_image)\n",
    "    pose_image = detect_pose(input_image)\n",
    "    mask,mask_image,mask_array = segment_image(input_image,inverse=True,additional_option='shoe')\n",
    "    mask2,mask_image2,mask_array2 = segment_image(input_image,inverse=False,additional_option='shoe')\n",
    "            \n",
    "    # if image.size[0] < image.size[0]:\n",
    "    #             sm_image=resize_images(image,sm_pose_image.size,aspect_ratio=sm_pose_image.size[0]/sm_pose_image.size[1])\n",
    "    #             sm_pose_image=resize_images(pose_image,sm_pose_image.size,aspect_ratio=None)\n",
    "    #             sm_mask=resize_images(mask_image,sm_pose_image.size,aspect_ratio=sm_pose_image.size[0]/sm_pose_image.size[1])\n",
    "        \n",
    "    # else:\n",
    "    sm_image=resize_images(image,target_size=image.size,aspect_ratio=None)\n",
    "    sm_pose_image=resize_images(pose_image,image.size,aspect_ratio=image.size[0]/image.size[1])\n",
    "    sm_mask=resize_images(mask_image,image.size,aspect_ratio=image.size[0]/image.size[1])\n",
    "    return sm_image, sm_pose_image, sm_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d759b7-6cd8-4440-959a-9aa9825ed1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def showImagesHorizontally(list_of_files, prompt,negative_prompt,model,time,height,width,controlnet_conditioning_scale,num_inference_steps, guidance_scale,seed,strength,output_path):\n",
    "        from matplotlib.pyplot import figure, imshow, axis\n",
    "        from matplotlib.image import imread\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        fig = figure(figsize=(10,5))\n",
    "        number_of_files = len(list_of_files)\n",
    "        for i in range(number_of_files):\n",
    "            a=fig.add_subplot(1,number_of_files,i+1)\n",
    "            image = (list_of_files[i])\n",
    "            imshow(image,cmap='Greys_r')\n",
    "            axis('off')\n",
    "\n",
    "        # Add text to the image\n",
    "        fig.text(0.5, 0.01, f\"Prompt: {prompt}       Neg_Prompt: {negative_prompt} \\n Model: {model}  Time(s): {np.round(time,2)}  Time(m): {np.round(time/60,2)}  height: {height}  width: {width}  cond_scale: {controlnet_conditioning_scale}  steps: {num_inference_steps}  guidance: {guidance_scale} strength: {strength} seed: {seed}\", ha='center', fontsize=10, color='black', wrap=True)\n",
    "        text_to_save = f\"\"\"\n",
    "        Prompt: {prompt} \n",
    "        Neg_Prompt: {negative_prompt}\n",
    "        Model: {model}\n",
    "        Time: {time}\n",
    "        height: {height}\n",
    "        width: {width}\n",
    "        cond_scale: {controlnet_conditioning_scale}\n",
    "        steps: {num_inference_steps}\n",
    "        guidance: {guidance_scale}\n",
    "        seed: {seed}\n",
    "        strength:{strength}\n",
    "        time: {time}\"\"\"\n",
    "        \n",
    "         # Save the text to a .txt file\n",
    "        with open(output_path+'.txt', 'w') as file:  # Specify the desired file name\n",
    "            file.write(text_to_save)  # Write the text to the file\n",
    "        \n",
    "        plt.tight_layout()  # Adjust the layout to prevent overlapping\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')  # Save the figure\n",
    "        plt.close(fig)  # Close the figure to free up memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233127cc-371d-43f7-8787-94af9cc00de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [sm_image.size,sm_pose_image.size,sm_mask.size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d0a5e7-09c7-4d62-a584-3113e94e36bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# showImagesHorizontally([sm_image,sm_pose_image,sm_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f56df2-84c1-4f88-82c9-600d0d68dc67",
   "metadata": {},
   "source": [
    "### run with 2 terminals: command `watch nvidia-smi` and `top`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ca3dd3-9a23-4147-a70f-aa8e2dbc8dd8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### DEV 1800-2500mb gpumem 60-85% Volatile GPU-Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e6c7e6-0a67-483e-bb28-58b6094ac0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import inspect\n",
    "# ## change flux inpainting pipeline to allow negative-prompt in OpenFlux\n",
    "# # PTH=(os.path.abspath(inspect.getfile(FluxControlNetInpaintPipeline)))\n",
    "# PTH2=(os.path.abspath(inspect.getfile(FluxImg2ImgPipeline)))\n",
    "# import shutil\n",
    "# shutil.copy('LookBuilderPipeline/LookBuilderPipeline/image_models/openflux/orig_pipeline_flux_controlnet_inpainting.py', PTH2)\n",
    "# # PTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9088ae-a666-4388-aa2e-3e206e56b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = FluxControlNetInpaintPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", controlnet=controlnet, torch_dtype=torch.bfloat16)\n",
    "\n",
    "# pipe.enable_model_cpu_offload()\n",
    "# pipe.text_encoder.to(torch.bfloat16)\n",
    "# pipe.enable_sequential_cpu_offload()\n",
    "\n",
    "# prompt='a photo realistic image of  black fashion model in atlanta georgia'\n",
    "# # negative_prompt=\"ugly, bad quality, bad anatomy, deformed body, deformed hands, deformed feet, deformed face, deformed clothing, deformed skin, bad skin, leggings, tights, stockings, pants, sleeves\",\n",
    "\n",
    "# pr = cProfile.Profile()\n",
    "# pr.enable()\n",
    "# start1 = time.time()\n",
    "\n",
    "# image_res = pipe(\n",
    "#                 prompt,\n",
    "#                 # negative_prompt=negative_prompt,\n",
    "#                 image=sm_image,\n",
    "#                 control_image=sm_pose_image,\n",
    "#                 control_mode=4,\n",
    "#                 # padding_mask_crop=32,\n",
    "#                 controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "#                 mask_image=sm_mask,\n",
    "#                 height=height,\n",
    "#                 width=width,\n",
    "#                 strength=0.99,\n",
    "#                 num_inference_steps=num_inference_steps,\n",
    "#                 guidance_scale=guidance_scale,\n",
    "#                 generator=generator,\n",
    "#                 # prompt_embeds=embeds,\n",
    "#                 # pooled_prompt_embeds=embeds,\n",
    "#             ).images[0]\n",
    "# end1 = time.time()\n",
    "# with open('CN_benchmark/dev_test.txt', 'w') as stream:\n",
    "#     stats = Stats(pr,stream=stream)\n",
    "#     stats.sort_stats('tottime').print_stats(50)\n",
    "\n",
    "# pr.disable()\n",
    "# image_res.save('CN_benchmark/dev_test.png')\n",
    "# image_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4f3de-d010-42ae-b811-68d1735e7258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = FluxControlNetInpaintPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", controlnet=controlnet, torch_dtype=torch.bfloat16)\n",
    "\n",
    "# pipe.enable_model_cpu_offload()\n",
    "# pipe.text_encoder.to(torch.bfloat16)\n",
    "# pipe.enable_sequential_cpu_offload()\n",
    "# prompts=['a photo realistic image of a fashion model on a beach',\n",
    "#         'a photo realistic image of a fashion model on a sailboat',\n",
    "#         'a photo realistic image of a fashion model in a temple']\n",
    "# for input_image in dir:\n",
    "#     for j,prompt in enumerate(prompts):\n",
    "#         sm_image, sm_pose_image, sm_mask= image_setup(input_image)\n",
    "#         if j==0:\n",
    "#             jj='beach'\n",
    "#         elif j==1:\n",
    "#             jj='boat'\n",
    "#         elif j==2:\n",
    "#             jj='temple'\n",
    "#         i=os.path.basename(input_image).split('.')[0]\n",
    "#         width,height=sm_image.size\n",
    "        \n",
    "#         num_inference_steps=10\n",
    "#         guidance_scale=7.5\n",
    "#         controlnet_conditioning_scale=0.99\n",
    "#         seed=420042\n",
    "#         generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
    "#         negative_prompt='ugly, bad quality, bad anatomy, deformed body, deformed hands, deformed feet, deformed face, deformed clothing, deformed skin, bad skin, leggings, tights, stockings, pants, sleeves'\n",
    "\n",
    "#         pr = cProfile.Profile()\n",
    "#         pr.enable()\n",
    "#         start1 = time.time()\n",
    "\n",
    "\n",
    "#         image_res = pipe(\n",
    "#                 prompt,\n",
    "#                 # negative_prompt=negative_prompt,\n",
    "#                 image=sm_image,\n",
    "#                 control_image=sm_pose_image,\n",
    "#                 control_mode=4,\n",
    "#                 padding_mask_crop=32,\n",
    "#                 controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "#                 mask_image=sm_mask,\n",
    "#                 height=height,\n",
    "#                 width=width,\n",
    "#                 strength=0.9,\n",
    "#                 num_inference_steps=num_inference_steps,\n",
    "#                 guidance_scale=guidance_scale,\n",
    "#                 generator=generator,\n",
    "#                 # prompt_embeds=embeds,\n",
    "#                 # pooled_prompt_embeds=embeds,\n",
    "#             ).images[0]\n",
    "#         end1 = time.time()\n",
    "#         with open('dev_benchmark/dev_test'+str(i)+'_'+str(jj)+'.txt', 'w') as stream:\n",
    "#             stats = Stats(pr,stream=stream)\n",
    "#             stats.sort_stats('tottime').print_stats(50)\n",
    "        \n",
    "#         pr.disable()\n",
    "#         # image_res.save('dev_benchmark/dev_test.png')\n",
    "#         # image_res\n",
    "\n",
    "        \n",
    "#         # image_res.save('dev_benchmark/dev_test_'+str(i)+'_'+str(jj)+'.png')\n",
    "#         filename='dev_benchmark/dev_bench_'+str(i)+'_'+str(jj)+'.png'\n",
    "#         showImagesHorizontally([sm_image,sm_mask,sm_pose_image,image_res], prompt,negative_prompt,'dev',height, width, controlnet_conditioning_scale,num_inference_steps,guidance_scale,generator,output_path=filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d4e23f-6b77-4d93-9f8d-e03a1f8a2f80",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### SCHENLL 2000-2500mb gpumem 55-80% Volatile GPU-Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc777fa-80a8-48c9-a093-4fe5bd02efd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = FluxControlNetInpaintPipeline.from_pretrained(\"black-forest-labs/FLUX.1-schnell\", controlnet=controlnet, torch_dtype=torch.bfloat16)\n",
    "\n",
    "# pipe.enable_model_cpu_offload()\n",
    "# pipe.text_encoder.to(torch.bfloat16)\n",
    "# pipe.enable_sequential_cpu_offload()\n",
    "\n",
    "# prompt='a photo realistic image of  black fashion model in atlanta georgia'\n",
    "# negative_prompt=\"ugly, bad quality, bad anatomy, deformed body, deformed hands, deformed feet, deformed face, deformed clothing, deformed skin, bad skin, leggings, tights, stockings, pants, sleeves\",\n",
    "\n",
    "# pr = cProfile.Profile()\n",
    "# pr.enable()\n",
    "# start1 = time.time()\n",
    "\n",
    "# image_res = pipe(\n",
    "#                 prompt,\n",
    "#                 # negative_prompt=negative_prompt,\n",
    "#                 image=sm_image,\n",
    "#                 control_image=sm_pose_image,\n",
    "#                 control_mode=4,\n",
    "#                 # padding_mask_crop=32,\n",
    "#                 controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "#                 mask_image=sm_mask,\n",
    "#                 height=height,\n",
    "#                 width=width,\n",
    "#                 strength=0.99,\n",
    "#                 num_inference_steps=num_inference_steps,\n",
    "#                 guidance_scale=guidance_scale,\n",
    "#                 generator=generator,\n",
    "#                 # prompt_embeds=embeds,\n",
    "#                 # pooled_prompt_embeds=embeds,\n",
    "#             ).images[0]\n",
    "# end1 = time.time()\n",
    "# with open('CN_benchmark/schnell_test.txt', 'w') as stream:\n",
    "#     stats = Stats(pr,stream=stream)\n",
    "#     stats.sort_stats('tottime').print_stats(50)\n",
    "\n",
    "# pr.disable()\n",
    "# image_res.save('CN_benchmark/schnell_test.png')\n",
    "# image_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29879bda-8359-4bac-ae36-cc3a853e9b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = FluxControlNetInpaintPipeline.from_pretrained(\"black-forest-labs/FLUX.1-schnell\", controlnet=controlnet, torch_dtype=torch.bfloat16)\n",
    "\n",
    "# pipe.enable_model_cpu_offload()\n",
    "# pipe.text_encoder.to(torch.bfloat16)\n",
    "# pipe.enable_sequential_cpu_offload()\n",
    "# prompts=['a photo realistic image of a fashion model on a beach',\n",
    "#         'a photo realistic image of a fashion model on a sailboat',\n",
    "#         'a photo realistic image of a fashion model in a temple']\n",
    "# for input_image in dir:\n",
    "#     for j,prompt in enumerate(prompts):\n",
    "#         sm_image, sm_pose_image, sm_mask= image_setup(input_image)\n",
    "        \n",
    "#         width,height=sm_image.size\n",
    "        \n",
    "#         num_inference_steps=30\n",
    "#         guidance_scale=7\n",
    "#         controlnet_conditioning_scale=1\n",
    "#         seed=42\n",
    "#         generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
    "#         negative_prompt='ugly, bad quality, bad anatomy, deformed body, deformed hands, deformed feet, deformed face, deformed clothing, deformed skin, bad skin, leggings, tights, stockings, pants, sleeves'\n",
    "#         i=os.path.basename(input_image).split('.')[0]\n",
    "#         if j==0:\n",
    "#             jj='beach'\n",
    "#         elif j==1:\n",
    "#             jj='boat'\n",
    "#         elif j==2:\n",
    "#             jj='temple'\n",
    "\n",
    "#         pr = cProfile.Profile()\n",
    "#         pr.enable()\n",
    "#         start1 = time.time()\n",
    "        \n",
    "\n",
    "#         image_res = pipe(\n",
    "#                 prompt,\n",
    "#                 # negative_prompt=negative_prompt,\n",
    "#                 image=sm_image,\n",
    "#                 control_image=sm_pose_image,\n",
    "#                 control_mode=4,\n",
    "#                 # padding_mask_crop=32,\n",
    "#                 controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "#                 mask_image=sm_mask,\n",
    "#                 height=height,\n",
    "#                 width=width,\n",
    "#                 strength=0.99,\n",
    "#                 num_inference_steps=num_inference_steps,\n",
    "#                 guidance_scale=guidance_scale,\n",
    "#                 generator=generator,\n",
    "#                 # prompt_embeds=embeds,\n",
    "#                 # pooled_prompt_embeds=embeds,\n",
    "#             ).images[0]\n",
    "#         end1 = time.time()\n",
    "#         with open('schnell_benchmark/schnell_test'+str(i)+'_'+str(jj)+'.txt', 'w') as stream:\n",
    "#             stats = Stats(pr,stream=stream)\n",
    "#             stats.sort_stats('tottime').print_stats(50)\n",
    "        \n",
    "#         pr.disable()\n",
    "#         # image_res.save('dev_benchmark/dev_test.png')\n",
    "#         # image_res\n",
    "\n",
    "\n",
    "        \n",
    "#         image_res.save('schnell_benchmark/schnell_test_'+str(i)+'_'+str(jj)+'.png')\n",
    "#         filename='schnell_benchmark/schnell_bench_'+str(i)+'_'+str(jj)+'.png'\n",
    "#         showImagesHorizontally([sm_image,sm_mask,sm_pose_image,image_res], prompt,negative_prompt,'schnell',height, width, controlnet_conditioning_scale,num_inference_steps,guidance_scale,generator,output_path=filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77287746-a45d-4a91-8431-e24ef9decc9a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### OpenFLUX 700-1200mb gpumem 60-85% Volatile GPU-Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51a0e86-5dc9-40ad-b73c-5937b03ad8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import inspect\n",
    "# ## change flux inpainting pipeline to allow negative-prompt in OpenFlux\n",
    "# # PTH=(os.path.abspath(inspect.getfile(FluxControlNetInpaintPipeline)))\n",
    "# PTH2=(os.path.abspath(inspect.getfile(FluxImg2ImgPipeline)))\n",
    "# import shutil\n",
    "# shutil.copy('LookBuilderPipeline/LookBuilderPipeline/image_models/openflux/cfg_pipeline_flux_controlnet_inpainting.py', PTH2)\n",
    "# # PTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d632c2e4-7421-4267-af2c-422d456b93ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import inspect\n",
    "# PTH=(os.path.abspath(inspect.getfile(FluxControlNetInpaintPipeline)))\n",
    "# PTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af6f538-2bf5-4f04-9fbd-e982cf103b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from diffusers import FluxControlNetInpaintPipeline\n",
    "\n",
    "# pipe = FluxControlNetInpaintPipeline.from_pretrained(\"ostris/OpenFLUX.1\", controlnet=controlnet, torch_dtype=torch.bfloat16)\n",
    "\n",
    "# pipe.enable_model_cpu_offload()\n",
    "# pipe.text_encoder.to(torch.bfloat16)\n",
    "# pipe.enable_sequential_cpu_offload()\n",
    "\n",
    "# prompt='a photo realistic image of  black fashion model in atlanta georgia'\n",
    "# negative_prompt=\"ugly, bad quality, bad anatomy, deformed body, deformed hands, deformed feet, deformed face, deformed clothing, deformed skin, bad skin, leggings, tights, stockings, pants, sleeves\",\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca21e4fc-ff5a-45fb-b28e-4e603c4a11b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_image=dir[2]\n",
    "# sm_image, sm_pose_image, sm_mask= image_setup(input_image)\n",
    "\n",
    "# height,width=sm_image.size\n",
    "# generator = torch.Generator(device=\"cuda\").manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90017921-c8ac-416f-8516-1e436b7d4d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pr = cProfile.Profile()\n",
    "# pr.enable()\n",
    "# start1 = time.time()\n",
    "\n",
    "# image_res = pipe(\n",
    "#                 prompt,\n",
    "#                 negative_prompt=negative_prompt,\n",
    "#                 image=sm_image,\n",
    "#                 control_image=sm_pose_image,\n",
    "#                 control_mode=4,\n",
    "#                 # padding_mask_crop=32,\n",
    "#                 controlnet_conditioning_scale=0.9,\n",
    "#                 mask_image=sm_mask,\n",
    "#                 height=height,\n",
    "#                 width=width,\n",
    "#                 strength=0.99,\n",
    "#                 num_inference_steps=20,\n",
    "#                 guidance_scale=8,\n",
    "#                 generator=generator,\n",
    "#                 # prompt_embeds=embeds,\n",
    "#                 # pooled_prompt_embeds=embeds,\n",
    "#             ).images[0]\n",
    "# end1 = time.time()\n",
    "# with open('openflux_benchmark/openflux_test.txt', 'w') as stream:\n",
    "#     stats = Stats(pr,stream=stream)\n",
    "#     stats.sort_stats('tottime').print_stats(50)\n",
    "\n",
    "# pr.disable()\n",
    "# image_res.save('openflux_benchmark/openflux_test.png')\n",
    "# image_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ae7128-79b0-4123-b427-3ae43c527c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = FluxControlNetInpaintPipeline.from_pretrained(\"ostris/OpenFLUX.1\", controlnet=controlnet, torch_dtype=torch.bfloat16)\n",
    "\n",
    "# pipe.enable_model_cpu_offload()\n",
    "# pipe.text_encoder.to(torch.bfloat16)\n",
    "# pipe.enable_sequential_cpu_offload()\n",
    "# prompts=['a photo realistic image of a fashion model on a beach',\n",
    "#         'a photo realistic image of a fashion model on a sailboat',\n",
    "#         'a photo realistic image of a fashion model in a temple']\n",
    "# for input_image in dir:\n",
    "#     for j,prompt in enumerate(prompts):\n",
    "#         sm_image, sm_pose_image, sm_mask= image_setup(input_image)\n",
    "#         i=os.path.basename(input_image).split('.')[0]\n",
    "#         if j==0:\n",
    "#             jj='beach'\n",
    "#         elif j==1:\n",
    "#             jj='boat'\n",
    "#         elif j==2:\n",
    "#             jj='temple'\n",
    "#         width,height=sm_image.size\n",
    "        \n",
    "#         num_inference_steps=30\n",
    "#         guidance_scale=7\n",
    "#         controlnet_conditioning_scale=1\n",
    "#         seed=42\n",
    "#         generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
    "#         negative_prompt='ugly, bad quality, bad anatomy, deformed body, deformed hands, deformed feet, deformed face, deformed clothing, deformed skin, bad skin, leggings, tights, stockings, pants, sleeves'\n",
    "        \n",
    "#         pr = cProfile.Profile()\n",
    "#         pr.enable()\n",
    "#         start1 = time.time()\n",
    "\n",
    "\n",
    "#         image_res = pipe(\n",
    "#                 prompt,\n",
    "#                 negative_prompt=negative_prompt,\n",
    "#                 image=sm_image,\n",
    "#                 control_image=sm_pose_image,\n",
    "#                 control_mode=4,\n",
    "#                 # padding_mask_crop=32,\n",
    "#                 controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "#                 mask_image=sm_mask,\n",
    "#                 height=height,\n",
    "#                 width=width,\n",
    "#                 strength=0.99,\n",
    "#                 num_inference_steps=num_inference_steps,\n",
    "#                 guidance_scale=guidance_scale,\n",
    "#                 generator=generator,\n",
    "#                 # prompt_embeds=embeds,\n",
    "#                 # pooled_prompt_embeds=embeds,\n",
    "#             ).images[0]\n",
    "#         end1 = time.time()\n",
    "#         with open('openflux_benchmark/openflux_test'+str(i)+'_'+str(jj)+'.txt', 'w') as stream:\n",
    "#             stats = Stats(pr,stream=stream)\n",
    "#             stats.sort_stats('tottime').print_stats(50)\n",
    "        \n",
    "#         pr.disable()\n",
    "\n",
    "\n",
    "#         image_res.save('openflux_benchmark/openflux_test_'+str(i)+'_'+str(jj)+'.png')\n",
    "#         filename='openflux_benchmark/openflux_bench_'+str(i)+'_'+str(jj)+'.png'\n",
    "#         showImagesHorizontally([sm_image,sm_mask,sm_pose_image,image_res], prompt,negative_prompt,'openflux',height, width, controlnet_conditioning_scale,num_inference_steps,guidance_scale,generator,output_path=filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6739c964-b1aa-4bd0-a59a-cc93f7e48192",
   "metadata": {},
   "source": [
    "#### QUANTIZED_OpenFLUX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37bbc1f-409b-48f4-bb54-0a6af6d44ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import snapshot_download\n",
    "# snapshot_download(repo_id=\"ostris/OpenFLUX.1\",local_dir='flux-fp8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6f2cb6d-a7b7-4ce5-bc0b-efaec35e9b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import FlowMatchEulerDiscreteScheduler, AutoencoderKL\n",
    "from diffusers.models.transformers.transformer_flux import FluxTransformer2DModel\n",
    "from diffusers.pipelines.flux.pipeline_flux import FluxPipeline\n",
    "from transformers import CLIPTextModel, CLIPTokenizer,T5EncoderModel, T5TokenizerFast\n",
    "dtype = torch.bfloat16\n",
    "scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained('flux-fp8', subfolder=\"scheduler\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=dtype)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=dtype)\n",
    "# text_encoder_2 = T5EncoderModel.from_pretrained('flux-fp8', subfolder=\"text_encoder_2\", torch_dtype=dtype)\n",
    "tokenizer_2 = T5TokenizerFast.from_pretrained('flux-fp8', subfolder=\"tokenizer_2\", torch_dtype=dtype)\n",
    "vae = AutoencoderKL.from_pretrained('flux-fp8', subfolder=\"vae\", torch_dtype=dtype)\n",
    "# transformer = FluxTransformer2DModel.from_pretrained('flux-fp8', subfolder=\"transformer\", torch_dtype=dtype)\n",
    "\n",
    "# Experimental: Try this to load in 4-bit for <16GB cards.\n",
    "#\n",
    "# from quanto import quantize, freeze, qint4, qint8, qfloat8\n",
    "# from optimum.quanto import QuantizedModelForCausalLM, qfloat8, quantize,freeze\n",
    "# pr = cProfile.Profile()\n",
    "# pr.enable()\n",
    "# start1 = time.time()\n",
    "# quantize(transformer, weights=qfloat8)\n",
    "# freeze(transformer)\n",
    "\n",
    "# quantize(text_encoder_2, weights=qfloat8)\n",
    "# freeze(text_encoder_2)\n",
    "# end1 = time.time()\n",
    "# with open('benchmark/Quanto_test.txt', 'w') as stream:\n",
    "#     stats = Stats(pr,stream=stream)\n",
    "#     stats.sort_stats('tottime').print_stats(50)\n",
    "\n",
    "# pr.disable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724dc120-a673-4055-a927-3df4042e9e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from safetensors.torch import save_file\n",
    "\n",
    "# save_file(text_encoder_2.state_dict(), 'model.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce52fe1-6344-4831-a80b-b0f20cb7d5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_encoder_2.save_pretrained('qopenflux/text_encoder_2_qfloat8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3526b844-2bd6-47c6-b7cd-f50bc28baf75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e95a3f44db64cb9bb899dd09154696c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# if glob.glob('qopenflux/text_encoder_2_qfloat8/*.safetensors'):\n",
    "    # text_encoder_2 = T5EncoderModel.from_pretrained('qopenflux', subfolder=\"text_encoder_2_qfloat8\", torch_dtype=dtype)\n",
    "# else:\n",
    "text_encoder_2 = T5EncoderModel.from_pretrained('flux-fp8', subfolder=\"text_encoder_2\", torch_dtype=dtype)\n",
    "quantize(text_encoder_2, weights=qfloat8)\n",
    "freeze(text_encoder_2)\n",
    "    # text_encoder_2.save_pretrained('qopenflux/text_encoder_2_qfloat8')\n",
    "\n",
    "# # if glob.glob('qopenflux/transformer_qfloat8/*.safetensors'):\n",
    "# try:\n",
    "#     transformer = FluxTransformer2DModel.from_pretrained('qopenflux', subfolder=\"transformer_qfloat8\", torch_dtype=dtype)\n",
    "# # else:\n",
    "# except:\n",
    "transformer = FluxTransformer2DModel.from_pretrained('flux-fp8', subfolder=\"transformer\", torch_dtype=dtype)\n",
    "quantize(transformer, weights=qfloat8)\n",
    "freeze(transformer)\n",
    "    # transformer.save_pretrained('qopenflux/transformer_qfloat8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfd45cd-98c5-4c8b-a336-5b0c417ae06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if glob.glob('qopenflux/text_encoder_2_int8/*.safetensors'):\n",
    "#     text_encoder_2 = T5EncoderModel.from_pretrained('qopenflux', subfolder=\"text_encoder_2_int8\", torch_dtype=dtype)\n",
    "# else:\n",
    "#     text_encoder_2 = T5EncoderModel.from_pretrained('flux-fp8', subfolder=\"text_encoder_2\", torch_dtype=dtype)\n",
    "#     quantize(text_encoder_2, weights=qint8)\n",
    "#     freeze(text_encoder_2)\n",
    "#     text_encoder_2.save_pretrained('qopenflux/text_encoder_2_int8')\n",
    "\n",
    "# # if glob.glob('qopenflux/transformer/*.safetensors'):\n",
    "# try:\n",
    "#     transformer = FluxTransformer2DModel.from_pretrained('qopenflux', subfolder=\"transformer_int8\", torch_dtype=dtype)\n",
    "# # else:\n",
    "# except:\n",
    "#     transformer = FluxTransformer2DModel.from_pretrained('flux-fp8', subfolder=\"transformer\", torch_dtype=dtype)\n",
    "#     quantize(transformer, weights=qint8)\n",
    "#     freeze(transformer)\n",
    "#     transformer.save_pretrained('qopenflux/transformer_int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6bb6a8-5bb4-4301-aa35-99b8d5882e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if glob.glob('qopenflux/text_encoder_2_int4/*.safetensors'):\n",
    "#     text_encoder_2 = T5EncoderModel.from_pretrained('qopenflux', subfolder=\"text_encoder_2_int4\", torch_dtype=dtype)\n",
    "# else:\n",
    "#     text_encoder_2 = T5EncoderModel.from_pretrained('flux-fp8', subfolder=\"text_encoder_2\", torch_dtype=dtype)\n",
    "#     quantize(text_encoder_2, weights=qint4)\n",
    "#     freeze(text_encoder_2)\n",
    "#     text_encoder_2.save_pretrained('qopenflux/text_encoder_2_int4')\n",
    "\n",
    "# try:\n",
    "#     transformer = FluxTransformer2DModel.from_pretrained('qopenflux', subfolder=\"text_encoder_2_int4\", torch_dtype=dtype)\n",
    "# except:\n",
    "#     transformer = FluxTransformer2DModel.from_pretrained('flux-fp8', subfolder=\"transformer\", torch_dtype=dtype)\n",
    "#     quantize(transformer, weights=qint4)\n",
    "#     freeze(transformer)\n",
    "#     transformer.save_pretrained('qopenflux/transformer_int4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656be037-ecc7-48df-a5d3-c10b7785c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_encoder_2.save_pretrained('qopenflux/text_encoder_2_int4',strict=False)\n",
    "# transformer.save_pretrained('qopenflux/transformer_int4',strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7dcfd5-7e77-49d5-bd1f-ad0cd63908b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, inspect, shutil\n",
    "# PTH=(os.path.abspath(inspect.getfile(FluxPipeline)))\n",
    "# target='/'.join(PTH.split('/')[:-1])+'/pipeline_flux_controlnet_inpainting.py'\n",
    "# source='LookBuilderPipeline/LookBuilderPipeline/image_models/openflux/cfg_pipeline_flux_controlnet_inpainting.py'\n",
    "# shutil.copy(source, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b71eda-8223-49c4-a0c5-eb772548ac54",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### QUANTIZED_OpenFLUX 22650mb gpumem 100% Volatile GPU-Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2412b52f-7e2b-42f7-9593-764a86ee8687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from importlib import reload\n",
    "# import diffusers\n",
    "# reload(diffusers)\n",
    "from diffusers import FluxControlNetInpaintPipeline #, FluxImg2ImgPipeline\n",
    "# dir[12:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dea4847-3e77-4d86-bf54-9fa7047fca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del pipe\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# from importlib import reload\n",
    "# import optimum.quanto\n",
    "# reload(optimum.quanto)\n",
    "\n",
    "# from optimum.quanto import qfloat8,qint4,qint8, quantize,freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4322c528-debb-4b51-b64a-04b0a98a312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe = FluxControlNetInpaintPipeline(\n",
    "    controlnet=controlnet,\n",
    "    scheduler=scheduler,\n",
    "    text_encoder=text_encoder,\n",
    "    tokenizer=tokenizer,\n",
    "    text_encoder_2=None,\n",
    "    tokenizer_2=tokenizer_2,\n",
    "    vae=vae,\n",
    "    transformer=None,\n",
    ")\n",
    "\n",
    "pipe.text_encoder_2 = text_encoder_2\n",
    "pipe.transformer = transformer\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "# # pipe.load_lora_weights('hugovntr/flux-schnell-realism', weight_name='schnell-realism_v1')\n",
    "# pipe.load_lora_weights('hugovntr/flux-schnell-realism', weight_name='schnell-realism_v2.3.safetensors', adapter_name=\"winner\")\n",
    "# from diffusers import EulerAncestralDiscreteScheduler\n",
    "# pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe3.scheduler.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659a0192-27af-4c4c-b51d-4275c6a4009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = FluxControlNetInpaintPipeline.from_pretrained(\"ostris/OpenFLUX.1\", controlnet=controlnet, torch_dtype=torch.bfloat16)\n",
    "\n",
    "# pipe.enable_model_cpu_offload()\n",
    "# pipe.text_encoder.to(torch.bfloat16)\n",
    "# pipe.enable_sequential_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df86915-1127-4c0e-8c6d-c9b45b5d153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import load_image  # For loading images\n",
    "from transformers import pipeline  # For using pre-trained models\n",
    "import numpy as np  # For numerical operations on arrays\n",
    "from PIL import Image, ImageOps  # For image manipulation\n",
    "\n",
    "\n",
    "# Initialize the segmentation model using a pre-trained model from Hugging Face\n",
    "segmenter = pipeline(model=\"mattmdjaga/segformer_b2_clothes\")\n",
    "\n",
    "\n",
    "def full_mask(original_image,flux_image):\n",
    "    seg_img = load_image(original_image)\n",
    "    segments = segmenter(seg_img)\n",
    "    segment_include = [\"Upper-clothes\", \"Skirt\", \"Pants\", \"Dress\", \"Belt\", \"Bag\", \"Scarf\",\"Left-shoe\",\"Right-shoe\"]\n",
    "        \n",
    "    seg_img = load_image(flux_image)\n",
    "    segments = segmenter(seg_img)\n",
    "    segment_include += [\"Hat\",\"Hair\",\"Sunglasses\",\"Face\",\"Left-leg\",\"Right-leg\",\"Left-arm\",\"Right-arm\"]\n",
    "        \n",
    "        \n",
    "    mask_list = [np.array(s['mask']) for s in segments if s['label'] not in segment_include]\n",
    "    final_mask = np.array(mask_list[0])\n",
    "    for mask in mask_list:\n",
    "        current_mask = np.array(mask)\n",
    "        final_mask = final_mask + current_mask\n",
    "        \n",
    "    final_array = final_mask.copy()\n",
    "    final_mask = Image.fromarray(final_mask)\n",
    "        # seg_img.putalpha(final_mask)\n",
    "        \n",
    "    mask = Image.new(\"L\", final_mask.size)\n",
    "    mask.paste(final_mask)\n",
    "    mask = ImageOps.invert(mask)\n",
    "    final_maskA = mask.point(lambda p: p > 128 and 255)\n",
    "    # mask_blurred = pipeline.mask_processor.blur(final_maskA, blur_factor=20)\n",
    "    return final_mask, final_maskA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc334ef7-e521-4d95-876e-f61cd8e3f3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt=[#'a photo realistic image of a fashion model at a party with curly brown hair and yelling something', \\\n",
    "#         # 'a photo realistic image of a fashion model with straight hair and hazel eyes and smiling showing white teeth at an ancient roman temple with many stone large columns and stone-paved streets at sunset']\n",
    "#         'A very tall and slender model with longer-than-average arms and neck poses looking straight into the camera. 1. Model characteristics: - Height: Very tall and slender - Arms: Longer than average - Neck: Longer than average - Facial features: Very sharp, almost masculine but elegant - Expression: Very intriguing - Makeup: Completely nude look - Hair: Wet and slicked back with a wide-toothed comb 2. Lighting and effects: - Eyes: Shining intensely with the flash light - Skin: Completely glossy, highlighting the lines from the flash - Background: Light gray with a vignette - Shadows: Strongly defined by the flash, giving the impression the model is floating - Optical distortion: Continued - Setting: Roman++ Ampetheatre++ - Camera settings: Aperture at f/2.8']\n",
    "# from compel import Compel, ReturnedEmbeddingsType\n",
    "# compel = Compel(tokenizer=[pipe.tokenizer, pipe.tokenizer_2] , text_encoder=[pipe.text_encoder, pipe.text_encoder_2], returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, requires_pooled=[False, True])\n",
    "# conditioning, pooled = compel(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f2524a-bda3-4348-8c72-017d4d553994",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23cf9ea-69f7-49e2-9120-9719ded95039",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=['a photo realistic image of a fashion model at a party with curly brown hair and yelling something']\n",
    "        # 'a photo realistic image of a fashion model with straight hair and hazel eyes and smiling showing white teeth at an ancient roman temple with many stone large columns and stone-paved streets at sunset']\n",
    "        # 'A very tall and slender model with longer-than-average arms and neck poses looking straight into the camera. 1. Model characteristics: - Height: Very tall and slender - Arms: Longer than average - Neck: Longer than average - Facial features: Very sharp, almost masculine but elegant - Expression: Very intriguing - Makeup: Completely nude look - Hair: Wet and slicked back with a wide-toothed comb 2. Lighting and effects: - Eyes: Shining intensely with the flash light - Skin: Completely glossy, highlighting the lines from the flash - Background: Light gray with a vignette - Shadows: Strongly defined by the flash, giving the impression the model is floating - Optical distortion: Continued - Camera settings: Aperture at f/2.8']\n",
    "        # 'a photo realistic image of a fashion model with curly blonde hair and blue eyes and smirking on a busy city street corner under tall skyscrapers and neon lights at night']\n",
    "\n",
    "\n",
    "num_inference_steps=40\n",
    "\n",
    "for input_image in dir:\n",
    "    for guidance_scale in [6,7,8,9]:\n",
    "    # for j,prompt in enumerate(prompts):\n",
    "        for controlnet_conditioning_scale in [1.0, 0.9, 0.7,0.5]:\n",
    "            for strength in [0.3, 0.7, 0.9]:\n",
    "\n",
    "\n",
    "                sm_image, sm_pose_image, sm_mask= image_setup(input_image)\n",
    "                i=os.path.basename(input_image).split('.')[0]\n",
    "                # if j==0:\n",
    "                jj='temple'\n",
    "                # elif j==1:\n",
    "                    # jj='city'\n",
    "                # elif j==2:\n",
    "                #     jj='boat'\n",
    "                width,height=sm_image.size\n",
    "                \n",
    "                # num_inference_steps=20\n",
    "                seed=42\n",
    "                \n",
    "        \n",
    "                generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
    "                negative_prompt='ugly, bad quality, bad anatomy, deformed body, deformed hands, deformed feet, deformed face, deformed clothing, deformed skin, bad skin, leggings, tights, stockings, pants, sleeves'\n",
    "                \n",
    "                # pr = cProfile.Profile()\n",
    "                # pr.enable()\n",
    "                start1 = time.time()\n",
    "        \n",
    "        \n",
    "                image_res = pipe(\n",
    "                        prompt,\n",
    "                        # prompt_embeds=conditioning,\n",
    "                        # pooled_prompt_embeds=pooled,\n",
    "                        negative_prompt=negative_prompt,\n",
    "                        image=sm_image,\n",
    "                        control_image=sm_pose_image,\n",
    "                        control_mode=4,\n",
    "                        padding_mask_crop=32,\n",
    "                        controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "                        mask_image=sm_mask,\n",
    "                        height=height,\n",
    "                        width=width,\n",
    "                        strength=strength,\n",
    "                        num_inference_steps=num_inference_steps,\n",
    "                        guidance_scale=guidance_scale,\n",
    "                        generator=generator,\n",
    "                        # prompt_embeds=embeds,\n",
    "                        # pooled_prompt_embeds=embeds,\n",
    "                    ).images[0]\n",
    "                end1 = time.time()\n",
    "                tt=end1-start1\n",
    "                # with open('qopenflux_benchmark/qopenflux_test'+str(i)+'_'+str(jj)+'.txt', 'w') as stream:\n",
    "                #     stats = Stats(pr,stream=stream)\n",
    "                #     stats.sort_stats('tottime').print_stats(50)\n",
    "                \n",
    "                # pr.disable()\n",
    "        \n",
    "        \n",
    "                image_res.save('qopenflux_benchmark/qopenflux_test_'+str(i)+'_'+str(jj)+'.png')\n",
    "                filename='qopenflux_benchmark/qopenflux_bench_guid_'+str(i)+'_'+str(jj)+str(guidance_scale)+'_cond_'+str(controlnet_conditioning_scale)+'_strength_'+str(strength)+'_ints_'+str(num_inference_steps)+'.png'\n",
    "                showImagesHorizontally([sm_image,sm_mask,sm_pose_image,image_res], prompt,negative_prompt,'openflux',tt,height, width, controlnet_conditioning_scale,num_inference_steps,guidance_scale,seed,strength,output_path=filename)\n",
    "\n",
    "\n",
    "        # final_mask, final_maskA =full_mask(sm_image,image_res)\n",
    "        # controlnet_conditioning_scale=0.9\n",
    "\n",
    "        # image_res2 = pipe(\n",
    "        #         prompt= 'a photo realistic image of a fashion model at an ancient roman temple with many stone large columns and stone-paved streets',\n",
    "        #         negative_prompt=negative_prompt,\n",
    "        #         image=image_res,\n",
    "        #         control_image=sm_pose_image,\n",
    "        #         control_mode=4,\n",
    "        #         # padding_mask_crop=32,\n",
    "        #         controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "        #         mask_image=final_mask,\n",
    "        #         height=height,\n",
    "        #         width=width,\n",
    "        #         strength=strength,\n",
    "        #         num_inference_steps=num_inference_steps,\n",
    "        #         guidance_scale=guidance_scale,\n",
    "        #         generator=generator,\n",
    "        #         # prompt_embeds=embeds,\n",
    "        #         # pooled_prompt_embeds=embeds,\n",
    "        #     ).images[0]\n",
    "        # end2 = time.time()\n",
    "        # tt2=end2-start1\n",
    "\n",
    "        # # image_res2.save('qopenflux_benchmark/qopenflux_test_'+str(i)+'_'+str(jj)+'.png')\n",
    "        # filename='qopenflux_benchmark/out_qopenflux_bench_guid_'+str(i)+'_'+str(jj)+str(guidance_scale)+'_cond_'+str(controlnet_conditioning_scale)+'_strength_'+str(strength)+'_ints_'+str(num_inference_steps)+'.png'\n",
    "        # showImagesHorizontally([sm_image,sm_mask,sm_pose_image,image_res2], prompt,negative_prompt,'openflux',tt,height, width, controlnet_conditioning_scale,num_inference_steps,guidance_scale,seed,strength,output_path=filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503c7422-7d88-4e09-b010-7b708ad4ab29",
   "metadata": {},
   "source": [
    "#### quanto-openflux NOCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa38240-6d08-436a-86a9-7a5ab7f94136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import FluxInpaintPipeline\n",
    "pipe2 = FluxInpaintPipeline(\n",
    "    scheduler=scheduler,\n",
    "    text_encoder=text_encoder,\n",
    "    tokenizer=tokenizer,\n",
    "    text_encoder_2=None,\n",
    "    tokenizer_2=tokenizer_2,\n",
    "    vae=vae,\n",
    "    transformer=None,\n",
    ")\n",
    "\n",
    "\n",
    "pipe2.text_encoder_2 = text_encoder_2\n",
    "pipe2.transformer = transformer\n",
    "pipe2.enable_model_cpu_offload()\n",
    "\n",
    "# pipe.save_pretrained('qopenflux')\n",
    "# transformer.save_pretrained('qopenflux/transformer/')\n",
    "# text_encoder_2.save_pretrained('qopenflux/text_encoder_2/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b9737d-3b00-4cc3-b9c3-03189f17b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prompts=['a photo realistic image of a fashion model on a sailing boat with blonde curly hair and blue eyes and a crooked nose and pale skin', \\\n",
    "#         'a photo realistic image of a fashion model at an ancient roman temple  with curly brown hair and hazel eyes and smiling showing white teeth',  \\\n",
    "#         'a photo realistic image of a fashion model on a busy city street corner with curly brown hair and hazel eyes and smiling showing white teeth']\n",
    "\n",
    "# input_image=dir[10]\n",
    "\n",
    "# guidance_scale=6\n",
    "# controlnet_conditioning_scale=0.7\n",
    "# strength=0.9\n",
    "# for j,prompt in enumerate(prompts):\n",
    "#     # for controlnet_conditioning_scale in [0.7,0.9]: #,0.99]:\n",
    "#     # for strength in [0.7,0.8,0.9,0.99]:\n",
    "#     for num_inference_steps in [30]:\n",
    "\n",
    "#         sm_image, sm_pose_image, sm_mask= image_setup(input_image)\n",
    "#         i=os.path.basename(input_image).split('.')[0]\n",
    "#         if j==0:\n",
    "#             jj='boat'\n",
    "#         elif j==1:\n",
    "#             jj='temple'\n",
    "#         elif j==2:\n",
    "#             jj='city'\n",
    "#         width,height=sm_image.size\n",
    "        \n",
    "#         # num_inference_steps=20\n",
    "#         seed=42\n",
    "        \n",
    "\n",
    "#         generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
    "#         negative_prompt='ugly, bad quality, bad anatomy, deformed body, deformed hands, deformed feet, deformed face, deformed clothing, deformed skin, bad skin, leggings, tights, stockings, pants, sleeves'\n",
    "        \n",
    "#         pr = cProfile.Profile()\n",
    "#         pr.enable()\n",
    "#         start1 = time.time()\n",
    "\n",
    "\n",
    "#         image_res = pipe2(\n",
    "#                 prompt,\n",
    "#                 # negative_prompt=negative_prompt,\n",
    "#                 image=sm_image,\n",
    "#                 # control_image=sm_pose_image,\n",
    "#                 # control_mode=4,\n",
    "#                 padding_mask_crop=32,\n",
    "#                 # controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "#                 mask_image=sm_mask,\n",
    "#                 height=height,\n",
    "#                 width=width,\n",
    "#                 strength=strength,\n",
    "#                 num_inference_steps=num_inference_steps,\n",
    "#                 guidance_scale=guidance_scale,\n",
    "#                 generator=generator,\n",
    "#                 # prompt_embeds=embeds,\n",
    "#                 # pooled_prompt_embeds=embeds,\n",
    "#             ).images[0]\n",
    "#         end1 = time.time()\n",
    "#         tt=end1-start1\n",
    "#         # with open('qopenflux_benchmark/qopenflux_test'+str(i)+'_'+str(jj)+'.txt', 'w') as stream:\n",
    "#         #     stats = Stats(pr,stream=stream)\n",
    "#         #     stats.sort_stats('tottime').print_stats(50)\n",
    "        \n",
    "#         pr.disable()\n",
    "\n",
    "\n",
    "#         # image_res.save('qopenflux_benchmark/qopenflux_test_'+str(i)+'_'+str(jj)+'.png')\n",
    "#         filename='qopenflux_benchmark/NOCN_qopenflux_bench_guid_'+str(i)+'_'+str(jj)+str(guidance_scale)+'_cond_'+str(controlnet_conditioning_scale)+'_strength_'+str(strength)+'_ints_'+str(num_inference_steps)+'.png'\n",
    "#         showImagesHorizontally([sm_image,sm_mask,sm_pose_image,image_res], prompt,negative_prompt,'openflux',tt,height, width, controlnet_conditioning_scale,num_inference_steps,guidance_scale,seed,strength,output_path=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00abefa-191f-4fad-9390-784b4eb670f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20c41fea-2ef7-4ed0-832a-4c0f1f539a50",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## SDXL bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772ab12b-993d-49ca-a37e-ef5c01e2753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# from random import shuffle\n",
    "# # dir=glob.glob('test-ai/orig/*')\n",
    "# # dir+=glob.glob('test-ai/jcrew_orig/*')\n",
    "# # dir=glob.glob('test-ai/paulo_orig/*')\n",
    "# # dir=glob.glob('test-ai/upscaled/*')\n",
    "# dir=glob.glob('test-ai/more_paulo/*')\n",
    "\n",
    "# shuffle(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df44ee5-ae24-493f-b7e9-0eec92e0636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f3d185-9680-48cb-9ea7-5f2506e9650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from diffusers import StableDiffusionXLControlNetInpaintPipeline, ControlNetModel, DDIMScheduler\n",
    "\n",
    "# controlnet_model3 = ControlNetModel.from_pretrained(\"xinsir/controlnet-union-sdxl-1.0\", torch_dtype=torch.float16, use_safetensors=True,)\n",
    "\n",
    "\n",
    "# pipe3 = StableDiffusionXLControlNetInpaintPipeline.from_pretrained(\n",
    "#     # \"stabilityai/stable-diffusion-xl-base-1.0\", controlnet=controlnet_model, \n",
    "# #     \"RunDiffusion/Juggernaut-XL-v9\", \n",
    "#         \"SG161222/RealVisXL_V5.0_Lightning\",\n",
    "\n",
    "# # pipe3 = StableDiffusionXLControlNetInpaintPipeline.from_single_file(\n",
    "#     # \"https://huggingface.co/RunDiffusion/Juggernaut-XL-v9/blob/main/Juggernaut-XL_v9_RunDiffusionPhoto_v2.safetensors\", \n",
    "#     controlnet=controlnet_model3,\n",
    "#     # vae=vae,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     # scheduler=ddim_scheduler,\n",
    "#     # scheduler=eulera_scheduler,\n",
    "# )\n",
    "# # model_path = \"TheLastBen/William_Eggleston_Style_SDXL\"\n",
    "\n",
    "\n",
    "# # pipe = pipe.to(device)\n",
    "# pipe3.text_encoder.to(torch.float16)\n",
    "# pipe3.controlnet.to(torch.float16)\n",
    "# # pipe3.unet.load_attn_procs(model_path, use_safetensors=True)\n",
    "# # pipe3.load_lora_weights(\"TheLastBen/William_Eggleston_Style_SDXL\",weight_name='wegg.safetensors', adapter_name=\"wegg\")\n",
    "\n",
    "# # pipe3.load_lora_weights('ntc-ai/SDXL-LoRA-slider.stylish-photoshoot', weight_name='stylish photoshoot.safetensors', adapter_name=\"style\")\n",
    "# # pipe3.load_lora_weights('ntc-ai/SDXL-LoRA-slider.Product-Photo', weight_name='Product Photo.safetensors', adapter_name=\"style\")\n",
    "\n",
    "# pipe3.to(\"cuda\")\n",
    "\n",
    "# # pipe3.load_lora_weights('ntc-ai/SDXL-LoRA-slider.extremely-detailed', weight_name='extremely detailed.safetensors', adapter_name=\"extremely detailed\")\n",
    "\n",
    "# # # Activate the LoRA\n",
    "# # pipe3.set_adapters([\"extremely detailed\"], adapter_weights=[3.0])\n",
    "\n",
    "# # pipe3.load_lora_weights('ntc-ai/SDXL-LoRA-slider.winner', weight_name='winner.safetensors', adapter_name=\"winner\")\n",
    "\n",
    "# # Activate the LoRA\n",
    "# # pipe3.set_adapters([\"winner\"], adapter_weights=[2.0])\n",
    "# # pipe.enable_model_cpu_offload()\n",
    "# # pipe3.set_adapters([\"style\"], adapter_weights=[3.0])\n",
    "# from diffusers import EulerAncestralDiscreteScheduler\n",
    "\n",
    "# pipe3.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe3.scheduler.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cea1c4-4714-4fa6-91a1-c57b12da28ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt=[#'a photo realistic image of a fashion model at a party with curly brown hair and yelling something', \\\n",
    "#         # 'a photo realistic image of a fashion model with straight hair and hazel eyes and smiling showing white teeth at an ancient roman temple with many stone large columns and stone-paved streets at sunset']\n",
    "#         'A very tall and slender model with longer-than-average arms and neck poses looking straight into the camera. 1. Model characteristics: - Height: tall and slender - Arms: Longer than average - Neck: Longer than average - Facial features: Very sharp, elegant - Expression: Very intriguing - Makeup: Completely nude look - Hair: Wet and slicked back with a wide-toothed comb 2. Lighting and effects: - Eyes: Shining intensely with the flash light - Skin: Completely glossy, highlighting the lines from the flash - Background: Roman Ampetheatre++ - Shadows: Strongly defined by the flash, giving the impression the model is floating - Optical distortion: Continued - Camera settings: Aperture at f/2.8']\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cf94ba-4f6d-434f-a489-28f2d6406b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from compel import Compel, ReturnedEmbeddingsType\n",
    "# compel = Compel(tokenizer=[pipe3.tokenizer, pipe3.tokenizer_2] , text_encoder=[pipe3.text_encoder, pipe3.text_encoder_2], returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, requires_pooled=[False, True])\n",
    "# conditioning, pooled = compel(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fdc95e-3aa1-4530-a156-c1c86a8464e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prompt=[#'a photo realistic image of a fashion model at a party with curly brown hair and yelling something', \\\n",
    "# #         # 'a photo realistic image of a fashion model with straight hair and hazel eyes and smiling showing white teeth at an ancient roman temple with many stone large columns and stone-paved streets at sunset']\n",
    "# #         'A very tall and slender model with longer-than-average arms and neck poses looking straight into the camera. 1. Model characteristics: - Height: Very tall and slender - Arms: Longer than average - Neck: Longer than average - Facial features: Very sharp, almost masculine but elegant - Expression: Very intriguing - Makeup: Completely nude look - Hair: Wet and slicked back with a wide-toothed comb 2. Lighting and effects: - Eyes: Shining intensely with the flash light - Skin: Completely glossy, highlighting the lines from the flash - Background: Light gray with a vignette - Shadows: Strongly defined by the flash, giving the impression the model is floating - Optical distortion: Continued - Camera settings: Aperture at f/2.8']\n",
    "\n",
    "# # 'a photo realistic image of a fashion model with curly blonde hair and blue eyes and smirking on a busy city street corner under tall skyscrapers and neon lights at night']\n",
    "\n",
    "\n",
    "# num_inference_steps=200\n",
    "# for strength in [0.3]:\n",
    "#     for guidance_scale in [10]:\n",
    "#     # for j,prompt in enumerate(prompts):\n",
    "#         for controlnet_conditioning_scale in [1.0]:\n",
    "\n",
    "#             # for input_image in dir:\n",
    "#             for input_image in dir[2:3]:\n",
    "\n",
    "\n",
    "#                 sm_image, sm_pose_image, sm_mask= image_setup(input_image)\n",
    "#                 i=os.path.basename(input_image).split('.')[0]\n",
    "#                 # if j==0:\n",
    "#                 jj='temple'\n",
    "#                 # elif j==1:\n",
    "#                     # jj='city'\n",
    "#                 # elif j==2:\n",
    "#                 #     jj='boat'\n",
    "#                 width,height=sm_image.size\n",
    "                \n",
    "#                 # num_inference_steps=20\n",
    "#                 seed=42\n",
    "                \n",
    "        \n",
    "#                 generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
    "#                 negative_prompt='ugly, bad quality, bad anatomy, deformed body, deformed hands, deformed feet, deformed face, deformed clothing, deformed skin, bad skin, leggings, tights, stockings, pants, sleeves'\n",
    "                \n",
    "#                 # pr = cProfile.Profile()\n",
    "#                 # pr.enable()\n",
    "#                 start1 = time.time()\n",
    "        \n",
    "        \n",
    "#                 image_res = pipe3(\n",
    "#                         # prompt,\n",
    "#                         prompt_embeds=conditioning,\n",
    "#                         pooled_prompt_embeds=pooled,\n",
    "#                         negative_prompt=negative_prompt,\n",
    "#                         image=sm_image,\n",
    "#                         control_image=sm_pose_image,\n",
    "#                         # control_mode=4,\n",
    "#                         padding_mask_crop=32,\n",
    "#                         controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "#                         mask_image=sm_mask,\n",
    "#                         # height=height,\n",
    "#                         # width=width,\n",
    "#                         strength=strength,\n",
    "#                         num_inference_steps=num_inference_steps,\n",
    "#                         guidance_scale=guidance_scale,\n",
    "#                         generator=generator,\n",
    "#                         # prompt_embeds=embeds,\n",
    "#                         # pooled_prompt_embeds=embeds,\n",
    "#                     ).images[0]\n",
    "#                 end1 = time.time()\n",
    "#                 tt=end1-start1\n",
    "#                 # with open('qopenflux_benchmark/qopenflux_test'+str(i)+'_'+str(jj)+'.txt', 'w') as stream:\n",
    "#                 #     stats = Stats(pr,stream=stream)\n",
    "#                 #     stats.sort_stats('tottime').print_stats(50)\n",
    "                \n",
    "#                 # pr.disable()\n",
    "        \n",
    "        \n",
    "#                 image_res.save('sdxl_benchmark/more_paulo/sdxl_test_'+str(i)+'_'+str(jj)+'.png')\n",
    "#                 filename='sdxl_benchmark/more_paulo/'+str(i)+'_'+str(jj)+'_guid_'+str(guidance_scale)+'_cond_'+str(controlnet_conditioning_scale)+'_strength_'+str(strength)+'_ints_'+str(num_inference_steps)+'.png'\n",
    "#                 showImagesHorizontally([sm_image,sm_mask,sm_pose_image,image_res], prompt,negative_prompt,'openflux',tt,height, width, controlnet_conditioning_scale,num_inference_steps,guidance_scale,seed,strength,output_path=filename)\n",
    "\n",
    "#                 torch.cuda.empty_cache()\n",
    "            \n",
    "#         # final_mask, final_maskA =full_mask(sm_image,image_res)\n",
    "#         # controlnet_conditioning_scale=0.9\n",
    "\n",
    "#         # image_res2 = pipe(\n",
    "#         #         prompt= 'a photo realistic image of a fashion model at an ancient roman temple with many stone large columns and stone-paved streets',\n",
    "#         #         negative_prompt=negative_prompt,\n",
    "#         #         image=image_res,\n",
    "#         #         control_image=sm_pose_image,\n",
    "#         #         control_mode=4,\n",
    "#         #         # padding_mask_crop=32,\n",
    "#         #         controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "#         #         mask_image=final_mask,\n",
    "#         #         height=height,\n",
    "#         #         width=width,\n",
    "#         #         strength=strength,\n",
    "#         #         num_inference_steps=num_inference_steps,\n",
    "#         #         guidance_scale=guidance_scale,\n",
    "#         #         generator=generator,\n",
    "#         #         # prompt_embeds=embeds,\n",
    "#         #         # pooled_prompt_embeds=embeds,\n",
    "#         #     ).images[0]\n",
    "#         # end2 = time.time()\n",
    "#         # tt2=end2-start1\n",
    "\n",
    "#         # # image_res2.save('qopenflux_benchmark/qopenflux_test_'+str(i)+'_'+str(jj)+'.png')\n",
    "#         # filename='qopenflux_benchmark/out_qopenflux_bench_guid_'+str(i)+'_'+str(jj)+str(guidance_scale)+'_cond_'+str(controlnet_conditioning_scale)+'_strength_'+str(strength)+'_ints_'+str(num_inference_steps)+'.png'\n",
    "#         # showImagesHorizontally([sm_image,sm_mask,sm_pose_image,image_res2], prompt,negative_prompt,'openflux',tt,height, width, controlnet_conditioning_scale,num_inference_steps,guidance_scale,seed,strength,output_path=filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdd6a59-7c00-454b-8997-e59f6a093726",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

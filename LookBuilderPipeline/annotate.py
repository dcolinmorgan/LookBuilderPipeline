from transformers import pipeline
from typing import Union, List
from PIL import Image


def annotate_images(image: Union[str, Image.Image, List[Union[str, Image.Image]]]):
    """
    Caption a single image or a list of images.

    Args:
        images (str, Image.Image, or List[Union[str, Image.Image]]): 
            A single image (as file path or PIL Image object) or a list of images.

    Returns:
        Union[Image.Image, List[Image.Image]]: The annotated image(s) as text.

    Raises:
        ValueError: If the input type is not recognized.
    """

    captioner = pipeline("image-to-text", model="Salesforce/blip-image-captioning-base")

    return captioner(image)[0]['generated_text']

def image_blip(self,image: Union[str, Image.Image, List[Union[str, Image.Image]]]):

    from transformers import AutoProcessor, Blip2ForConditionalGeneration
    import torch

    # processor = AutoProcessor.from_pretrained("Salesforce/blip2-opt-2.7b")
    # model = Blip2ForConditionalGeneration.from_pretrained("Salesforce/blip2-opt-2.7b", torch_dtype=torch.bfloat16).to(self.device)

    processor = AutoProcessor.from_pretrained("llava-hf/llava-1.5-7b-hf")

    model = Blip2ForConditionalGeneration.from_pretrained("Salesforce/blip2-opt-2.7b", torch_dtype=torch.bfloat16).to(self.device)

    
    # prompt = "Question: Are there any deformities or irregularities in this image? Answer:"
    prompt = "Question: Can you tell this image was generated by AI? Answer:"

    
    inputs = processor(images=image, text=prompt, return_tensors="pt").to(self.device, torch.bfloat16)
    
    generated_ids = model.generate(**inputs, max_new_tokens=10)
    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
    return generated_text


def image_llava(self,image: Union[str, Image.Image, List[Union[str, Image.Image]]]):
    import requests
    from PIL import Image
    
    import torch
    from transformers import AutoProcessor, LlavaForConditionalGeneration
    
    model_id = "llava-hf/llava-1.5-13b-hf"
    model = LlavaForConditionalGeneration.from_pretrained(
        model_id, 
        torch_dtype=torch.float16, 
        low_cpu_mem_usage=True, 
        # load_in_4bit=True, 
        # use_flash_attention_2=True
    ).to('cpu')
    
    processor = AutoProcessor.from_pretrained(model_id)
    
    # Define a chat history and use `apply_chat_template` to get correctly formatted prompt
    # Each value in "content" has to be a list of dicts with types ("text", "image") 
    conversation = [
        {
    
          "role": "user",
          "content": [
              {"type": "text", "text": "Can you tell this image was generated by AI?"},
              {"type": "image"},
            ],
        },
    ]
    prompt = processor.apply_chat_template(chat_template,conversation, add_generation_prompt=True)
    
    inputs = processor(images=image, text=prompt, return_tensors='pt').to(0, torch.float16)
    
    output = model.generate(**inputs, max_new_tokens=200, do_sample=False)
    return (processor.decode(output[0][2:], skip_special_tokens=True))


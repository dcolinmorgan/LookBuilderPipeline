from transformers import pipeline
from typing import Union, List
from PIL import Image


def annotate_images(image: Union[str, Image.Image, List[Union[str, Image.Image]]]):
    """
    Caption a single image or a list of images.

    Args:
        images (str, Image.Image, or List[Union[str, Image.Image]]): 
            A single image (as file path or PIL Image object) or a list of images.

    Returns:
        Union[Image.Image, List[Image.Image]]: The annotated image(s) as text.

    Raises:
        ValueError: If the input type is not recognized.
    """

    captioner = pipeline("image-to-text", model="Salesforce/blip-image-captioning-base")

    return captioner(image)[0]['generated_text']

def image_answers(self,image: Union[str, Image.Image, List[Union[str, Image.Image]]]):

    from transformers import AutoProcessor, Blip2ForConditionalGeneration
    import torch

    processor = AutoProcessor.from_pretrained("Salesforce/blip2-opt-2.7b")
    model = Blip2ForConditionalGeneration.from_pretrained("Salesforce/blip2-opt-2.7b", torch_dtype=torch.bfloat16).to(self.device)

    # prompt = "Question: Are there any deformities or irregularities in this image? Answer:"
    prompt = "Question: Can you tell this image was generated by AI? Answer:"

    
    inputs = processor(image, text=prompt, return_tensors="pt").to(self.device, torch.bfloat16)
    
    generated_ids = model.generate(**inputs, max_new_tokens=10)
    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
    return generated_text
